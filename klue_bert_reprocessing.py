#!/usr/bin/env python3
"""
2ë‹¨ê³„: KLUE-BERTë¡œ ë°ì´í„° ì¬ì²˜ë¦¬
ê¸°ì¡´ koBERT ë°ì´í„°ë¥¼ KLUE-BERTë¡œ ë³€í™˜í•˜ê³  ë¼ë²¨ë§ ì¤€ë¹„
"""

import json
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict, Tuple
import glob
import re

def setup_klue_bert():
    """KLUE-BERT í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ì„¤ì •"""
    print("ğŸ”„ KLUE-BERT ì„¤ì • ì¤‘...")
    
    try:
        # KLUE-BERT í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
        
        # ëª¨ë¸ë„ ë¡œë“œ (ë‚˜ì¤‘ì— ì„ë² ë”© ì¶”ì¶œìš©)
        model = AutoModel.from_pretrained("klue/bert-base")
        
        print("âœ… KLUE-BERT ë¡œë“œ ì™„ë£Œ!")
        print(f"   Vocab í¬ê¸°: {tokenizer.vocab_size:,}")
        print(f"   íŠ¹ìˆ˜ í† í°: {tokenizer.special_tokens_map}")
        
        return tokenizer, model
        
    except Exception as e:
        print(f"âŒ KLUE-BERT ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ í•´ê²° ë°©ë²•:")
        print("   pip install transformers torch")
        print("   ë˜ëŠ” ì¸í„°ë„· ì—°ê²° í™•ì¸")
        return None, None

def find_kobert_files():
    """koBERT í† í¬ë‚˜ì´ì¦ˆ íŒŒì¼ë“¤ ìë™ ê²€ìƒ‰"""
    import glob
    import os
    
    print("\nğŸ” koBERT í† í¬ë‚˜ì´ì¦ˆ íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
    
    # tokenized_answers*.json íŒ¨í„´ìœ¼ë¡œ íŒŒì¼ ê²€ìƒ‰
    pattern = "tokenized_answers*.json"
    files = glob.glob(pattern)
    
    if not files:
        print("âŒ tokenized_answers*.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # íŒŒì¼ëª… ì •ë ¬ (ìˆ«ì ìˆœì„œëŒ€ë¡œ)
    def extract_number(filename):
        import re
        match = re.search(r'tokenized_answers(\d+)\.json', filename)
        return int(match.group(1)) if match else 0
    
    files.sort(key=extract_number)
    
    print(f"âœ… ë°œê²¬ëœ íŒŒì¼: {len(files)}ê°œ")
    for i, file in enumerate(files[:5]):  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
        print(f"   {i+1}. {file}")
    if len(files) > 5:
        print(f"   ... (ì´ {len(files)}ê°œ)")
    
    return files

def load_original_data():
    """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (32ê°œ íŒŒì¼ ì²˜ë¦¬)"""
    print("\nğŸ“‚ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    # koBERT íŒŒì¼ë“¤ ì°¾ê¸°
    kobert_files = find_kobert_files()
    if not kobert_files:
        return [], []
    
    all_original_answers = []
    all_kobert_data = []
    
    for i, file_path in enumerate(kobert_files):
        try:
            print(f"ğŸ“„ ë¡œë”© ì¤‘: {file_path} ({i+1}/{len(kobert_files)})")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                kobert_data = json.load(f)
            
            # ì›ë³¸ ë‹µë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            original_answers = [sample["original_answer"] for sample in kobert_data]
            
            all_original_answers.extend(original_answers)
            all_kobert_data.append({
                "file_path": file_path,
                "data": kobert_data,
                "sample_count": len(kobert_data)
            })
            
            print(f"   âœ… {len(kobert_data)}ê°œ ìƒ˜í”Œ ë¡œë“œ")
            
        except Exception as e:
            print(f"   âŒ {file_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue
    
    print(f"\nğŸ“Š ì „ì²´ ë¡œë“œ ê²°ê³¼:")
    print(f"   ì´ íŒŒì¼ ìˆ˜: {len(all_kobert_data)}")
    print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {len(all_original_answers)}")
    
    # ì˜ˆì‹œ ì¶œë ¥
    if all_original_answers:
        print("\nğŸ“ ì›ë³¸ ë‹µë³€ ì˜ˆì‹œ:")
        for i, answer in enumerate(all_original_answers[:3]):
            print(f"   {i+1}. {answer}")
    
    return all_original_answers, all_kobert_data

def reprocess_with_klue_bert_batch(tokenizer, kobert_files_data: List[Dict]) -> List[Dict]:
    """KLUE-BERTë¡œ íŒŒì¼ë³„ ë°°ì¹˜ ì²˜ë¦¬"""
    print(f"\nğŸ”„ KLUE-BERTë¡œ {len(kobert_files_data)}ê°œ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")
    
    processed_files = []
    total_samples = 0
    
    for file_idx, file_info in enumerate(kobert_files_data):
        file_path = file_info["file_path"]
        kobert_data = file_info["data"]
        sample_count = file_info["sample_count"]
        
        print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {file_path} ({file_idx+1}/{len(kobert_files_data)})")
        print(f"   ìƒ˜í”Œ ìˆ˜: {sample_count}ê°œ")
        
        # ì›ë³¸ ë‹µë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        original_answers = [sample["original_answer"] for sample in kobert_data]
        
        # KLUE-BERTë¡œ ì¬ì²˜ë¦¬
        klue_samples = []
        
        for i, text in enumerate(original_answers):
            if (i + 1) % 10 == 0 or i == len(original_answers) - 1:
                print(f"   ì§„í–‰ë¥ : {i+1}/{len(original_answers)} ({(i+1)/len(original_answers)*100:.1f}%)")
            
            # KLUE-BERT í† í¬ë‚˜ì´ì§•
            encoding = tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=128,
                return_tensors='pt',
                return_token_type_ids=True,
                return_attention_mask=True
            )
            
            # í† í° ë¦¬ìŠ¤íŠ¸ ìƒì„±
            tokens = tokenizer.tokenize(text)
            full_tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'].squeeze().tolist())
            
            # ë°ì´í„° êµ¬ì„±
            sample = {
                "original_sample_id": kobert_data[i].get("sample_id", i + 1),
                "klue_sample_id": total_samples + i + 1,
                "original_answer": text,
                
                # KLUE-BERT í† í¬ë‚˜ì´ì§• ê²°ê³¼
                "tokens": tokens,
                "full_tokens": full_tokens,
                
                # ëª¨ë¸ ì…ë ¥ìš© ë°ì´í„°
                "input_ids": encoding['input_ids'].squeeze().tolist(),
                "attention_mask": encoding['attention_mask'].squeeze().tolist(),
                "token_type_ids": encoding['token_type_ids'].squeeze().tolist(),
                
                # í†µê³„ ì •ë³´
                "token_count": len(tokens),
                "full_token_count": len(full_tokens),
                "char_count": len(text),
                "compression_ratio": len(tokens) / len(text) if text else 0,
                
                # UNK í† í° ë¶„ì„
                "unk_count": sum(1 for token in tokens if '[UNK]' in token),
                "unk_ratio": sum(1 for token in tokens if '[UNK]' in token) / len(tokens) * 100 if tokens else 0,
                
                # ë¼ë²¨ë§ ì¤€ë¹„
                "labels": ["O"] * len(tokens),
                "labeling_ready": True,
                
                # ë©”íƒ€ë°ì´í„°
                "source_file": file_path,
                "processed_with": "KLUE-BERT"
            }
            
            klue_samples.append(sample)
        
        # íŒŒì¼ë³„ ê²°ê³¼ ì €ì¥
        file_result = {
            "source_file": file_path,
            "output_file": file_path.replace("tokenized_answers", "KLUE_tokenized_answers"),
            "samples": klue_samples,
            "sample_count": len(klue_samples),
            "avg_tokens": sum(s["token_count"] for s in klue_samples) / len(klue_samples) if klue_samples else 0,
            "avg_unk_ratio": sum(s["unk_ratio"] for s in klue_samples) / len(klue_samples) if klue_samples else 0
        }
        
        processed_files.append(file_result)
        total_samples += len(klue_samples)
        
        print(f"   âœ… ì™„ë£Œ: í‰ê·  {file_result['avg_tokens']:.1f} í† í°, UNK {file_result['avg_unk_ratio']:.1f}%")
    
    print(f"\nğŸ‰ ì „ì²´ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   ì²˜ë¦¬ëœ íŒŒì¼: {len(processed_files)}ê°œ")
    print(f"   ì´ ìƒ˜í”Œ: {total_samples}ê°œ")
    
    return processed_files

def compare_before_after(kobert_data: List[Dict], klue_data: List[Dict]):
    """ì¬ì²˜ë¦¬ ì „í›„ ë¹„êµ"""
    print("\nğŸ“Š ì¬ì²˜ë¦¬ ì „í›„ ë¹„êµ")
    print("=" * 60)
    
    if not kobert_data or not klue_data:
        print("âŒ ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì „ì²´ í†µê³„
    kobert_avg_tokens = sum(len(sample["tokens"]) for sample in kobert_data) / len(kobert_data)
    klue_avg_tokens = sum(sample["token_count"] for sample in klue_data) / len(klue_data)
    
    kobert_total_unk = sum(sample["tokens"].count("[UNK]") for sample in kobert_data)
    kobert_total_tokens = sum(len(sample["tokens"]) for sample in kobert_data)
    kobert_unk_ratio = (kobert_total_unk / kobert_total_tokens * 100) if kobert_total_tokens > 0 else 0
    
    klue_total_unk = sum(sample["unk_count"] for sample in klue_data)
    klue_total_tokens = sum(sample["token_count"] for sample in klue_data)
    klue_unk_ratio = (klue_total_unk / klue_total_tokens * 100) if klue_total_tokens > 0 else 0
    
    print(f"{'í•­ëª©':<15} {'koBERT (ê¸°ì¡´)':<15} {'KLUE-BERT (ì‹ ê·œ)':<18} {'ê°œì„ ë„':<10}")
    print("-" * 60)
    print(f"{'í‰ê·  í† í° ìˆ˜':<15} {kobert_avg_tokens:<15.1f} {klue_avg_tokens:<18.1f} {'-':<10}")
    print(f"{'UNK í† í° ë¹„ìœ¨':<15} {kobert_unk_ratio:<15.1f}% {klue_unk_ratio:<18.1f}% {(kobert_unk_ratio-klue_unk_ratio):<10.1f}%â†“")
    
    # ìƒ˜í”Œë³„ ìƒì„¸ ë¹„êµ (ì²˜ìŒ 3ê°œ)
    print(f"\nğŸ” ìƒ˜í”Œë³„ ìƒì„¸ ë¹„êµ:")
    for i in range(min(3, len(kobert_data), len(klue_data))):
        kobert_sample = kobert_data[i]
        klue_sample = klue_data[i]
        
        print(f"\n--- ìƒ˜í”Œ {i+1} ---")
        print(f"ì›ë³¸: {klue_sample['original_answer'][:50]}...")
        print(f"koBERT  í† í°: {kobert_sample['tokens'][:8]}...")
        print(f"KLUE-BERT í† í°: {klue_sample['tokens'][:8]}...")
        print(f"ê°œì„ : {len(kobert_sample['tokens'])} â†’ {klue_sample['token_count']} í† í°")

def analyze_labeling_readiness(klue_data: List[Dict]):
    """ë¼ë²¨ë§ ì¤€ë¹„ ìƒíƒœ ë¶„ì„"""
    print(f"\nğŸ·ï¸ ë¼ë²¨ë§ ì¤€ë¹„ ìƒíƒœ ë¶„ì„")
    print("=" * 40)
    
    total_samples = len(klue_data)
    total_tokens = sum(sample["token_count"] for sample in klue_data)
    avg_tokens = total_tokens / total_samples if total_samples > 0 else 0
    
    # UNK í† í° ë¶„ì„
    high_unk_samples = [sample for sample in klue_data if sample["unk_ratio"] > 10]
    
    print(f"ì´ ìƒ˜í”Œ ìˆ˜: {total_samples}")
    print(f"ì´ í† í° ìˆ˜: {total_tokens:,}")
    print(f"í‰ê·  í† í° ìˆ˜: {avg_tokens:.1f}")
    print(f"ê³ UNK ìƒ˜í”Œ: {len(high_unk_samples)}ê°œ ({len(high_unk_samples)/total_samples*100:.1f}%)")
    
    # ë¼ë²¨ë§ ë‚œì´ë„ í‰ê°€
    if avg_tokens < 30:
        difficulty = "ğŸŸ¢ ì‰¬ì›€"
    elif avg_tokens < 50:
        difficulty = "ğŸŸ¡ ë³´í†µ"
    else:
        difficulty = "ğŸ”´ ì–´ë ¤ì›€"
    
    print(f"ë¼ë²¨ë§ ë‚œì´ë„: {difficulty}")
    
    # ì˜ˆìƒ ë¼ë²¨ë§ ì‹œê°„
    estimated_time_per_sample = avg_tokens * 3  # í† í°ë‹¹ 3ì´ˆ ê°€ì •
    total_estimated_time = estimated_time_per_sample * total_samples / 60  # ë¶„ ë‹¨ìœ„
    
    print(f"ì˜ˆìƒ ë¼ë²¨ë§ ì‹œê°„: {total_estimated_time:.1f}ë¶„ ({total_estimated_time/60:.1f}ì‹œê°„)")
    
    return {
        "total_samples": total_samples,
        "avg_tokens": avg_tokens,
        "estimated_time_minutes": total_estimated_time,
        "difficulty": difficulty,
        "high_unk_samples": len(high_unk_samples)
    }

def save_klue_data_batch(processed_files: List[Dict]) -> List[str]:
    """KLUE-BERT ì²˜ë¦¬ ë°ì´í„° íŒŒì¼ë³„ ì €ì¥"""
    print(f"\nğŸ’¾ KLUE-BERT ë°ì´í„° íŒŒì¼ë³„ ì €ì¥ ì¤‘...")
    
    saved_files = []
    
    for file_info in processed_files:
        output_file = file_info["output_file"]
        samples = file_info["samples"]
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(samples, f, ensure_ascii=False, indent=2)
            
            file_size = len(json.dumps(samples, ensure_ascii=False)) / 1024
            
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_file}")
            print(f"   ìƒ˜í”Œ ìˆ˜: {len(samples)}ê°œ")
            print(f"   íŒŒì¼ í¬ê¸°: {file_size:.1f} KB")
            print(f"   í‰ê·  í† í°: {file_info['avg_tokens']:.1f}ê°œ")
            print(f"   í‰ê·  UNK: {file_info['avg_unk_ratio']:.1f}%")
            print()
            
            saved_files.append(output_file)
            
        except Exception as e:
            print(f"âŒ {output_file} ì €ì¥ ì‹¤íŒ¨: {e}")
            continue
    
    print(f"ğŸ‰ íŒŒì¼ë³„ ì €ì¥ ì™„ë£Œ!")
    print(f"   ì €ì¥ëœ íŒŒì¼: {len(saved_files)}ê°œ")
    
    return saved_files

def create_batch_summary(processed_files: List[Dict], saved_files: List[str]):
    """ë°°ì¹˜ ì²˜ë¦¬ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
    print(f"\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ìš”ì•½ ë³´ê³ ì„œ")
    print("=" * 60)
    
    total_samples = sum(file_info["sample_count"] for file_info in processed_files)
    total_files = len(processed_files)
    
    if total_samples > 0:
        overall_avg_tokens = sum(file_info["avg_tokens"] * file_info["sample_count"] for file_info in processed_files) / total_samples
        overall_avg_unk = sum(file_info["avg_unk_ratio"] * file_info["sample_count"] for file_info in processed_files) / total_samples
    else:
        overall_avg_tokens = 0
        overall_avg_unk = 0
    
    print(f"ğŸ“ˆ ì „ì²´ í†µê³„:")
    print(f"   ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜: {total_files}")
    print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {total_samples:,}")
    print(f"   ì „ì²´ í‰ê·  í† í° ìˆ˜: {overall_avg_tokens:.1f}")
    print(f"   ì „ì²´ í‰ê·  UNK ë¹„ìœ¨: {overall_avg_unk:.1f}%")
    
    print(f"\nğŸ“ íŒŒì¼ë³„ ìƒì„¸:")
    print(f"{'ì›ë³¸ íŒŒì¼':<25} {'ì¶œë ¥ íŒŒì¼':<25} {'ìƒ˜í”Œìˆ˜':<8} {'í‰ê· í† í°':<10} {'UNK%':<8}")
    print("-" * 80)
    
    for file_info in processed_files[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
        source = file_info["source_file"][:24]
        output = file_info["output_file"][:24]
        samples = file_info["sample_count"]
        avg_tokens = file_info["avg_tokens"]
        avg_unk = file_info["avg_unk_ratio"]
        
        print(f"{source:<25} {output:<25} {samples:<8} {avg_tokens:<10.1f} {avg_unk:<8.1f}")
    
    if len(processed_files) > 10:
        print(f"... (ì´ {len(processed_files)}ê°œ íŒŒì¼)")
    
    # ìš”ì•½ íŒŒì¼ ì €ì¥
    summary = {
        "processing_date": str(pd.Timestamp.now()),
        "total_files": total_files,
        "total_samples": total_samples,
        "overall_avg_tokens": overall_avg_tokens,
        "overall_avg_unk_ratio": overall_avg_unk,
        "file_details": processed_files,
        "saved_files": saved_files
    }
    
    with open("KLUE_batch_processing_summary.json", 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ìš”ì•½ ë³´ê³ ì„œ ì €ì¥: KLUE_batch_processing_summary.json")

def create_labeling_template(klue_data: List[Dict], sample_count: int = 1):
    """ë¼ë²¨ë§ í…œí”Œë¦¿ ìƒì„± (ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„)"""
    print(f"\nğŸ“‹ ë¼ë²¨ë§ í…œí”Œë¦¿ ìƒì„± ì¤‘... (ìƒ˜í”Œ {sample_count}ê°œ)")
    
    template = {
        "labeling_instructions": {
            "í‚¤ì›Œë“œ ìœ í˜•": [
                "ì¸ë¬¼: ê°€ì¡±, ë‚¨í¸, ë”¸, ì•„ë“¤, ì–´ë¨¸ë‹ˆ, ì¹œêµ¬",
                "ì¥ì†Œ: ë³‘ì›, ê³µì›, ì§‘, ì—¬í–‰ì§€, ìƒì ", 
                "ì‹œê°„: ë´„, ì—¬ë¦„, ì‘ë…„, ì–´ì œ, ì•„ì¹¨",
                "í™œë™: ì‚°ì±…, ìš”ë¦¬, ì—¬í–‰, ìš´ë™, ë§Œë‚¨",
                "ê°ì •: í–‰ë³µ, ìŠ¬í””, ê¸°ì¨, ê·¸ë¦¬ì›€",
                "ìì—°: ë²šê½ƒ, ë°”ëŒ, ë‹¬, ë³„"
            ],
            "ë¼ë²¨ ê·œì¹™": {
                "B-KEY": "í‚¤ì›Œë“œ ì‹œì‘",
                "I-KEY": "í‚¤ì›Œë“œ ë‚´ë¶€ (2ë²ˆì§¸ í† í°ë¶€í„°)",
                "O": "í‚¤ì›Œë“œ ì•„ë‹˜"
            }
        },
        "samples_to_label": []
    }
    
    for i in range(min(sample_count, len(klue_data))):
        sample = klue_data[i]
        
        labeling_sample = {
            "sample_id": sample["sample_id"],
            "original_answer": sample["original_answer"],
            "tokens": sample["tokens"],
            "labels": sample["labels"],  # ê¸°ë³¸ê°’ Oë¡œ ì„¤ì •ë¨
            "labeling_status": "pending",
            "keywords_to_find": []  # ë¼ë²¨ë§ ì‹œ ì±„ìš°ê¸°
        }
        
        template["samples_to_label"].append(labeling_sample)
    
    # í…œí”Œë¦¿ ì €ì¥
    template_filename = "labeling_template.json"
    with open(template_filename, 'w', encoding='utf-8') as f:
        json.dump(template, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ë¼ë²¨ë§ í…œí”Œë¦¿ ì €ì¥: {template_filename}")
    return template_filename

def main():
    """2ë‹¨ê³„ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - 32ê°œ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬"""
    print("ğŸ¯ 2ë‹¨ê³„: KLUE-BERT ë°ì´í„° ì¬ì²˜ë¦¬ (32ê°œ íŒŒì¼ ë°°ì¹˜)")
    print("=" * 60)
    
    # 1. KLUE-BERT ì„¤ì •
    tokenizer, model = setup_klue_bert()
    if not tokenizer:
        return
    
    # 2. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (32ê°œ íŒŒì¼)
    all_original_answers, kobert_files_data = load_original_data()
    if not kobert_files_data:
        return
    
    # 3. KLUE-BERTë¡œ ë°°ì¹˜ ì¬ì²˜ë¦¬
    processed_files = reprocess_with_klue_bert_batch(tokenizer, kobert_files_data)
    
    # 4. íŒŒì¼ë³„ ì €ì¥
    saved_files = save_klue_data_batch(processed_files)
    
    # 5. ë°°ì¹˜ ì²˜ë¦¬ ìš”ì•½
    create_batch_summary(processed_files, saved_files)
    
    # 6. ì™„ë£Œ ë³´ê³ ì„œ
    print(f"\nğŸ‰ 32ê°œ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 50)
    
    if processed_files:
        total_samples = sum(f["sample_count"] for f in processed_files)
        avg_tokens = sum(f["avg_tokens"] * f["sample_count"] for f in processed_files) / total_samples if total_samples > 0 else 0
        avg_unk = sum(f["avg_unk_ratio"] * f["sample_count"] for f in processed_files) / total_samples if total_samples > 0 else 0
        
        print(f"âœ… ì²˜ë¦¬ëœ íŒŒì¼: {len(processed_files)}ê°œ")
        print(f"âœ… ì´ ìƒ˜í”Œ: {total_samples:,}ê°œ")
        print(f"âœ… í‰ê·  í† í° ìˆ˜: {avg_tokens:.1f}ê°œ")
        print(f"âœ… í‰ê·  UNK ë¹„ìœ¨: {avg_unk:.1f}%")
        print(f"âœ… ì €ì¥ëœ íŒŒì¼: {len(saved_files)}ê°œ")
    
    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
    for saved_file in saved_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
        print(f"   - {saved_file}")
    if len(saved_files) > 5:
        print(f"   - ... (ì´ {len(saved_files)}ê°œ)")
    
    print(f"\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ (3ë‹¨ê³„):")
    print(f"1. KLUE_tokenized_answers*.json íŒŒì¼ë“¤ í™•ì¸")
    print(f"2. í‚¤ì›Œë“œ ë¼ë²¨ë§ ì‘ì—… ì¤€ë¹„")
    print(f"3. í† í° í’ˆì§ˆ ê²€ì¦")
    
    # ì²« ë²ˆì§¸ íŒŒì¼ ìƒ˜í”Œ ë³´ê¸°
    if processed_files and processed_files[0]["samples"]:
        sample = processed_files[0]["samples"][0]
        print(f"\nğŸ’¡ ì²˜ë¦¬ ê²°ê³¼ ì˜ˆì‹œ ({processed_files[0]['output_file']}):")
        print(f"ì›ë³¸: {sample['original_answer'][:50]}...")
        print(f"í† í°: {sample['tokens'][:8]}...")
        print(f"í† í° ìˆ˜: {sample['token_count']} (UNK: {sample['unk_count']})")
    
    print(f"\nğŸ¯ ì„±ê³µ! ì´ì œ KLUE-BERT í† í¬ë‚˜ì´ì¦ˆ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    return processed_files, saved_files

if __name__ == "__main__":
    main()