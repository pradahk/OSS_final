#!/usr/bin/env python3
"""
KLUE-BERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸°
í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
import json
import glob
import os
from typing import List, Optional, Dict, Tuple
import streamlit as st
from utils.constants import MAX_KEYWORDS_PER_ANSWER

class KeywordBERTModel(nn.Module):
    """KLUE-BERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸"""
    
    def __init__(self, num_labels=3, dropout_rate=0.1):
        super().__init__()
        self.bert = AutoModel.from_pretrained("klue/bert-base")
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        
    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )
        
        sequence_output = outputs.last_hidden_state
        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output)
        
        result = {"logits": logits}
        
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            result["loss"] = loss
            
        return result

# ê¸°ì¡´ í›ˆë ¨ëœ ëª¨ë¸ì´ ìˆìœ¼ë¯€ë¡œ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¬ì •ì˜ ë¶ˆí•„ìš”
# .pt íŒŒì¼ì—ì„œ ì§ì ‘ ë¡œë“œí•˜ì—¬ ì‚¬ìš©

class KeywordExtractor:
    """í‚¤ì›Œë“œ ì¶”ì¶œê¸° í´ë˜ìŠ¤ (ë¹„ìƒ í”Œëœ ë¡œì§ ì œê±°)"""
    
    def __init__(self, model_path: Optional[str] = None):
        """
        Args:
            model_path: ëª¨ë¸ ê²½ë¡œ. Noneì´ë©´ ìµœì‹  ëª¨ë¸ ìë™ ê²€ìƒ‰
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None
        self.label2id = {"O": 0, "B-KEY": 1, "I-KEY": 2}
        self.id2label = {v: k for k, v in self.label2id.items()}
        self.max_keywords = MAX_KEYWORDS_PER_ANSWER
        
        # ëª¨ë¸ ë¡œë“œ (ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ)
        self.load_model(model_path)
    
    def find_latest_model(self) -> str:
        """ê°€ì¥ ìµœê·¼ì— í›ˆë ¨ëœ ëª¨ë¸ ì°¾ê¸°. ì—†ìœ¼ë©´ FileNotFoundError ë°œìƒ"""
        # klue_keyword_extractor_* í´ë”ë“¤ ì°¾ê¸°
        model_dirs = glob.glob("klue_keyword_extractor_*")
        
        if not model_dirs:
            # best_model_*.pt íŒŒì¼ë“¤ ì°¾ê¸°
            model_files = glob.glob("best_model_*.pt")
            if model_files:
                # ê°€ì¥ ìµœì‹  íŒŒì¼ ì„ íƒ
                latest_file = max(model_files, key=os.path.getctime)
                return latest_file
            raise FileNotFoundError("í›ˆë ¨ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ ë˜ëŠ” íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ê°€ì¥ ìµœì‹  í´ë” ì„ íƒ
        latest_dir = max(model_dirs, key=os.path.getctime)
        return latest_dir
   
    def load_model(self, model_path: Optional[str] = None):
        """state_dict(.pt)ë¥¼ ë¡œë“œí•˜ì—¬ ëª¨ë¸ì„ ì¡°ë¦½í•©ë‹ˆë‹¤."""
        try:
            if model_path is None:
                model_path = self.find_latest_model()
            
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

            # --- ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ì…ë‹ˆë‹¤ ---
            # 1. ë¨¼ì € ëª¨ë¸ì˜ ë¹ˆ ì„¤ê³„ë„ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.
            self.model = KeywordBERTModel(num_labels=len(self.label2id))
            
            # 2. .pt íŒŒì¼ì—ì„œ 'ë¶€í’ˆ ìƒì(state_dict)'ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
            state_dict = torch.load(model_path, map_location=self.device)
            
            # 3. ë¹ˆ ì„¤ê³„ë„ì— ë¶ˆëŸ¬ì˜¨ ë¶€í’ˆë“¤ì„ ì¡°ë¦½í•©ë‹ˆë‹¤.
            self.model.load_state_dict(state_dict)
            
            # 4. ì´ì œ ì™„ì „í•œ ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜í•˜ê³ , ì¥ì¹˜ë¡œ ë³´ëƒ…ë‹ˆë‹¤.
            self.model.to(self.device)
            self.model.eval()
            # ------------------------------------
            
            # í† í¬ë‚˜ì´ì €ëŠ” KLUE-BERT ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©
            self.tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
            
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ë° ì¡°ë¦½ ì™„ë£Œ: {model_path}")
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.model = None
            raise

    def extract_keywords(self, text: str, max_keywords: Optional[int] = None) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ëª¨ë¸ ì „ìš©)"""
        # ì´ˆê¸°í™” ì‹œì ì— ëª¨ë¸ ë¡œë“œê°€ ë³´ì¥ë˜ë¯€ë¡œ, ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸
        if self.model is None or self.tokenizer is None:
            st.error("í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        if not text or not text.strip():
            return []
        
        try:
            # í† í¬ë‚˜ì´ì§•
            encoding = self.tokenizer(
                text,
                return_tensors='pt',
                truncation=True,
                padding=True,
                max_length=128
            )
            
            # GPUë¡œ ì´ë™
            input_ids = encoding['input_ids'].to(self.device)
            attention_mask = encoding['attention_mask'].to(self.device)
            
            # ì˜ˆì¸¡
            with torch.no_grad():
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                predictions = torch.argmax(outputs['logits'], dim=-1)
            
            # í† í° ê°€ì ¸ì˜¤ê¸°
            tokens = self.tokenizer.tokenize(text)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords_from_predictions(
                tokens, 
                predictions[0][1:len(tokens)+1],  # íŠ¹ìˆ˜ í† í° ì œì™¸
                max_keywords or self.max_keywords
            )
            
            return keywords
            
        except Exception as e:
            # ì¶”ë¡  ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ, ì‚¬ìš©ìì—ê²Œ ì•Œë¦¬ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            st.warning(f"âš ï¸ ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def _extract_keywords_from_predictions(self, tokens: List[str], predictions: torch.Tensor, 
                                            max_keywords: int) -> List[str]:
        """ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords_found = []
        current_keyword = ""
        
        for token, pred_id in zip(tokens, predictions):
            # ì´ë¯¸ ìµœëŒ€ ê°œìˆ˜ë§Œí¼ í‚¤ì›Œë“œë¥¼ ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨
            if len(keywords_found) >= max_keywords:
                break
            
            label = self.id2label[pred_id.item()]
            clean_token = token.replace('##', '')
            
            if label == 'B-KEY':
                if current_keyword:  # ì´ì „ í‚¤ì›Œë“œ ì™„ë£Œ
                    keywords_found.append(current_keyword)
                    if len(keywords_found) >= max_keywords:
                        break
                current_keyword = clean_token
                
            elif label == 'I-KEY' and current_keyword:
                current_keyword += clean_token
                
            else:
                if current_keyword:  # í‚¤ì›Œë“œ ì™„ë£Œ
                    keywords_found.append(current_keyword)
                    current_keyword = ""
        
        # ë§ˆì§€ë§‰ í‚¤ì›Œë“œ ì²˜ë¦¬
        if current_keyword and len(keywords_found) < max_keywords:
            keywords_found.append(current_keyword)
        
        return keywords_found
    
    # def calculate_keyword_similarity(self, keywords1: List[str], keywords2: List[str]) -> float:
    #     """ë‘ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1)"""
    #     if not keywords1 or not keywords2:
    #         return 0.0
        
    #     set1 = set(k.lower() for k in keywords1)
    #     set2 = set(k.lower() for k in keywords2)
        
    #     intersection = set1.intersection(set2)
    #     union = set1.union(set2)
        
    #     if not union:
    #         return 0.0
        
    #     similarity = len(intersection) / len(union)
    #     return similarity
    
    # def get_matching_keywords(self, keywords1: List[str], keywords2: List[str]) -> Tuple[List[str], List[str], List[str]]:
    #     """ë‘ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë¹„êµí•˜ì—¬ ì¼ì¹˜/ë¶ˆì¼ì¹˜ í‚¤ì›Œë“œ ë°˜í™˜
        
    #     Returns:
    #         (ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ, keywords1ì—ë§Œ ìˆëŠ” í‚¤ì›Œë“œ, keywords2ì—ë§Œ ìˆëŠ” í‚¤ì›Œë“œ)
    #     """
    #     set1 = set(k.lower() for k in keywords1)
    #     set2 = set(k.lower() for k in keywords2)
        
    #     matching = list(set1.intersection(set2))
    #     only_in_1 = list(set1 - set2)
    #     only_in_2 = list(set2 - set1)
        
    #     return matching, only_in_1, only_in_2

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_keyword_extractor = None

def get_keyword_extractor() -> Optional[KeywordExtractor]:
    """í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜. ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ None ë°˜í™˜."""
    global _keyword_extractor
    if _keyword_extractor is None:
        try:
            # KeywordExtractor ì´ˆê¸°í™” ì‹œë„. ì‹¤íŒ¨í•˜ë©´ ì˜ˆì™¸ ë°œìƒ
            _keyword_extractor = KeywordExtractor()
        except Exception as e:
            # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ _keyword_extractorëŠ” Noneìœ¼ë¡œ ìœ ì§€ë¨
            st.error(f"í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            return None
            
    return _keyword_extractor

def test_keyword_extraction():
    """í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    st.subheader("ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    extractor = get_keyword_extractor()
    
    if extractor is None:
        st.error("í‚¤ì›Œë“œ ì¶”ì¶œê¸°ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    test_text = st.text_area(
        "í…ŒìŠ¤íŠ¸í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        value="ê°€ì¡±ë“¤ê³¼ ê°”ë˜ ì œì£¼ë„ ì—¬í–‰ì´ ê°€ì¥ ê¸°ì–µì— ë‚¨ì•„ìš”. í•´ë³€ì—ì„œ ë†€ê³  ìœ ì±„ê½ƒë„ ë³´ê³  í‘ë¼ì§€ë„ ë¨¹ì–´ì„œ í–‰ë³µí–ˆì–´ìš”.",
        height=100
    )
    
    if st.button("í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"):
        if test_text.strip():
            with st.spinner("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                keywords = extractor.extract_keywords(test_text.strip())
                
                if keywords:
                    st.success(f"âœ… ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(keywords)}")
                else:
                    st.warning("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    st.title("ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œê¸° í…ŒìŠ¤íŠ¸")
    test_keyword_extraction()