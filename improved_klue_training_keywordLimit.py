#!/usr/bin/env python3
"""
ê°œì„ ëœ 4ë‹¨ê³„: KLUE-BERT í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ í•™ìŠµ
- ë§¤ë²ˆ ì´ˆê¸°í™” ë° ìƒˆë¡œìš´ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
- ì´ì „ í•™ìŠµ ë°ì´í„° ìë™ ì •ë¦¬
"""

import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, precision_recall_fscore_support
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple
import glob
import re
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import os
import shutil
from datetime import datetime

def cleanup_previous_training():
    """ì´ì „ í•™ìŠµ ê²°ê³¼ ì •ë¦¬"""
    print("ğŸ§¹ ì´ì „ í•™ìŠµ ê²°ê³¼ ì •ë¦¬ ì¤‘...")
    
    # ì‚­ì œí•  íŒŒì¼/í´ë” íŒ¨í„´ë“¤
    patterns_to_clean = [
        "best_model*.pt",
        "training_history*.png", 
        "klue_keyword_extractor*",
        "*.pt"  # ëª¨ë“  PyTorch ëª¨ë¸ íŒŒì¼
    ]
    
    cleaned_count = 0
    
    for pattern in patterns_to_clean:
        files = glob.glob(pattern)
        for file_path in files:
            try:
                if os.path.isfile(file_path):
                    os.remove(file_path)
                    print(f"   ğŸ—‘ï¸ íŒŒì¼ ì‚­ì œ: {file_path}")
                    cleaned_count += 1
                elif os.path.isdir(file_path):
                    shutil.rmtree(file_path)
                    print(f"   ğŸ—‘ï¸ í´ë” ì‚­ì œ: {file_path}")
                    cleaned_count += 1
            except Exception as e:
                print(f"   âš ï¸ ì‚­ì œ ì‹¤íŒ¨ {file_path}: {e}")
    
    if cleaned_count > 0:
        print(f"âœ… {cleaned_count}ê°œ íŒŒì¼/í´ë” ì •ë¦¬ ì™„ë£Œ")
    else:
        print("âœ… ì •ë¦¬í•  íŒŒì¼ ì—†ìŒ")

def generate_timestamp_suffix():
    """íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ íŒŒì¼ëª… ìƒì„±"""
    return datetime.now().strftime("%Y%m%d_%H%M%S")

def load_labeled_data():
    """ë¼ë²¨ë§ëœ KLUE ë°ì´í„° ë¡œë“œ ë° í™•ì¥""" # <<< ì£¼ì„ ë³€ê²½
    print("ğŸ“‚ ë¼ë²¨ë§ëœ KLUE ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    # ë¼ë²¨ë§ëœ KLUE íŒŒì¼ë“¤ ì°¾ê¸°
    labeled_files = glob.glob("KLUE_tokenized_answers*_labeled.json")
    
    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ê³  í•¨ìˆ˜ ì¢…ë£Œ
    if not labeled_files:
        print("âŒ KLUE_tokenized_answers*_labeled.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    print(f"âœ… ë°œê²¬ëœ ë¼ë²¨ë§ íŒŒì¼: {len(labeled_files)}ê°œ")
    
    all_labeled_data = []
    total_samples = 0
    total_keywords = 0
    
    # ê¸°ì¡´ê³¼ ë™ì¼: ëª¨ë“  JSON íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ all_labeled_dataì— ì¶”ê°€
    for file_path in sorted(labeled_files):
        print(f"ğŸ“„ ë¡œë”© ì¤‘: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                samples = json.load(f)
            
            file_keywords = 0
            valid_samples = []
            
            for sample in samples:
                # (ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
                if not all(key in sample for key in ["tokens", "labels", "input_ids", "attention_mask"]):
                    continue
                if len(sample["tokens"]) != len(sample["labels"]):
                    continue
                valid_labels = {"O", "B-KEY", "I-KEY"}
                if not all(label in valid_labels for label in sample["labels"]):
                    continue
                
                keyword_count = sum(1 for label in sample["labels"] if label.startswith('B-'))
                file_keywords += keyword_count
                
                sample["keyword_count"] = keyword_count
                sample["labeled"] = True
                
                valid_samples.append(sample)
            
            all_labeled_data.extend(valid_samples)
            total_keywords += file_keywords
            # total_samplesëŠ” ë‚˜ì¤‘ì— í•œë²ˆì— ê³„ì‚°í•˜ë¯€ë¡œ ì—¬ê¸°ì„œ ì œì™¸
            
            print(f"   âœ… {len(valid_samples)}ê°œ ìœ íš¨ ìƒ˜í”Œ, {file_keywords}ê°œ í‚¤ì›Œë“œ")
            
        except Exception as e:
            print(f"   âŒ {file_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue
            
    # <<< ì´ ì•„ë˜ì— ë°ì´í„° í™•ì¥ ë¡œì§ì„ ì¶”ê°€í•©ë‹ˆë‹¤ >>>
    
    # 1. í™•ì¥í•  ëª©í‘œ ë°ì´í„° ê°œìˆ˜ ì„¤ì •
    TARGET_COUNT = 1000
    
    # 2. ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ê°€ ìˆê³ , ê·¸ ìˆ˜ê°€ ëª©í‘œì¹˜ë³´ë‹¤ ì ì€ ê²½ìš°ì—ë§Œ í™•ì¥ ìˆ˜í–‰
    if all_labeled_data and len(all_labeled_data) < TARGET_COUNT:
        print(f"\nğŸ”¬ ì›ë³¸ ë°ì´í„° {len(all_labeled_data)}ê°œë¥¼ {TARGET_COUNT}ê°œë¡œ í™•ì¥í•©ë‹ˆë‹¤...")
        
        expanded_data = []
        for i in range(TARGET_COUNT):
            # ì›ë³¸ ë°ì´í„°ë¥¼ ìˆœí™˜í•˜ë©° ë³µì‚¬
            base_sample = all_labeled_data[i % len(all_labeled_data)]
            sample = base_sample.copy()
            # í•„ìš”í•˜ë‹¤ë©´ ë³µì œëœ ìƒ˜í”Œì— ê³ ìœ  ID ë¶€ì—¬
            sample["sample_id"] = f"expanded_{i+1}"
            expanded_data.append(sample)
        
        # í™•ì¥ëœ ë°ì´í„°ë¡œ ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ë¥¼ êµì²´
        all_labeled_data = expanded_data
        print(f"âœ… ë°ì´í„° í™•ì¥ ì™„ë£Œ!")

    # <<< ì—¬ê¸°ê¹Œì§€ ë°ì´í„° í™•ì¥ ë¡œì§ >>>


    # ì´ì œë¶€í„°ì˜ ëª¨ë“  í†µê³„ëŠ” í™•ì¥ëœ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.
    total_samples = len(all_labeled_data)
    if total_samples == 0:
        print("\n- ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
        
    total_keywords = sum(s.get("keyword_count", 0) for s in all_labeled_data)
    
    print(f"\nğŸ“Š ë¼ë²¨ë§ ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    print(f"   ì´ ìƒ˜í”Œ: {total_samples:,}ê°œ")
    print(f"   ì´ í‚¤ì›Œë“œ: {total_keywords:,}ê°œ")
    if total_samples > 0:
        print(f"   í‰ê·  í‚¤ì›Œë“œ/ìƒ˜í”Œ: {total_keywords/total_samples:.1f}ê°œ")
    
    # ë¼ë²¨ ë¶„í¬ í™•ì¸
    if all_labeled_data:
        all_labels = [label for sample in all_labeled_data for label in sample["labels"]]
        
        label_counts = pd.Series(all_labels).value_counts()
        print(f"\nğŸ“ˆ ë¼ë²¨ ë¶„í¬:")
        for label, count in label_counts.items():
            print(f"   {label}: {count:,}ê°œ ({count/len(all_labels)*100:.1f}%)")
        
        # ì˜ˆì‹œ ì¶œë ¥
        print(f"\nğŸ’¡ ë¼ë²¨ë§ ì˜ˆì‹œ (ì²« ë²ˆì§¸ ìƒ˜í”Œ):")
        # ... (ì˜ˆì‹œ ì¶œë ¥ ì½”ë“œëŠ” ë™ì¼) ...

    return all_labeled_data

''' def create_example_labeled_data():
    """ì˜ˆì‹œ ë¼ë²¨ë§ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)"""
    print("ğŸ”§ ì˜ˆì‹œ ë¼ë²¨ë§ ë°ì´í„° ìƒì„± ì¤‘...")
    
    example_data = [
        {
            "original_answer": "ë´„ì—ëŠ” ê°€ì¡±ê³¼ ë²šê½ƒ êµ¬ê²½ì„ ê°”ì–´ìš”",
            "tokens": ["ë´„", "##ì—ëŠ”", "ê°€ì¡±", "##ê³¼", "ë²šê½ƒ", "êµ¬ê²½", "##ì„", "ê°”ì–´ìš”"],
            "labels": ["B-KEY", "I-KEY", "B-KEY", "I-KEY", "B-KEY", "O", "O", "O"],  # ìˆ˜ì •ëœ ë¼ë²¨ë§
            "input_ids": [2, 1234, 5678, 2345, 6789, 3456, 7890, 4567, 3],
            "attention_mask": [1, 1, 1, 1, 1, 1, 1, 1, 1],
            "token_type_ids": [0, 0, 0, 0, 0, 0, 0, 0, 0],
            "keyword_count": 3,
            "labeled": True
        },
        {
            "original_answer": "ë‚¨í¸ê³¼ í•¨ê»˜ ë³‘ì›ì— ê°”ìŠµë‹ˆë‹¤",
            "tokens": ["ë‚¨í¸", "##ê³¼", "í•¨ê»˜", "ë³‘ì›", "##ì—", "ê°”ìŠµë‹ˆë‹¤"],
            "labels": ["B-KEY", "I-KEY", "O", "B-KEY", "I-KEY", "O"],  # ìˆ˜ì •ëœ ë¼ë²¨ë§
            "input_ids": [2, 8901, 4567, 8901, 2345, 6789, 3],
            "attention_mask": [1, 1, 1, 1, 1, 1, 1],
            "token_type_ids": [0, 0, 0, 0, 0, 0, 0],
            "keyword_count": 2,
            "labeled": True
        },
        {
            "original_answer": "ì–´ë¨¸ë‹ˆì™€ ì‹œì¥ì—ì„œ ê³¼ì¼ì„ ìƒ€ì–´ìš”",
            "tokens": ["ì–´ë¨¸ë‹ˆ", "##ì™€", "ì‹œì¥", "##ì—ì„œ", "ê³¼ì¼", "##ì„", "ìƒ€ì–´ìš”"],
            "labels": ["B-KEY", "I-KEY", "B-KEY", "I-KEY", "B-KEY", "I-KEY", "O"],  # ìˆ˜ì •ëœ ë¼ë²¨ë§
            "input_ids": [2, 3456, 7890, 1234, 5678, 9012, 3456, 3],
            "attention_mask": [1, 1, 1, 1, 1, 1, 1, 1],
            "token_type_ids": [0, 0, 0, 0, 0, 0, 0, 0],
            "keyword_count": 3,
            "labeled": True
        }
    ]
    
    # ë°ì´í„° í™•ì¥ (í•™ìŠµì— ì¶©ë¶„í•œ ì–‘ìœ¼ë¡œ)
    expanded_data = []
    for i in range(1000):  # 1000ê°œë¡œ í™•ì¥
        base_sample = example_data[i % len(example_data)]
        sample = base_sample.copy()
        sample["sample_id"] = i + 1
        expanded_data.append(sample)
    
    print(f"âœ… ì˜ˆì‹œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(expanded_data)}ê°œ ìƒ˜í”Œ")
    return expanded_data
    '''

class KeywordDataset(Dataset):
    """í‚¤ì›Œë“œ ì¶”ì¶œìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, samples: List[Dict], max_length: int = 128):
        self.samples = samples
        self.max_length = max_length
        
        # ë¼ë²¨ ë§¤í•‘ (ë‹¨ìˆœ í‚¤ì›Œë“œ ì¶”ì¶œ)
        self.label2id = {"O": 0, "B-KEY": 1, "I-KEY": 2}
        self.id2label = {v: k for k, v in self.label2id.items()}
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        input_ids = sample["input_ids"][:self.max_length]
        attention_mask = sample["attention_mask"][:self.max_length] 
        
        # ë¼ë²¨ ë³€í™˜ (í† í° ê°œìˆ˜ì— ë§ì¶¤)
        token_labels = sample["labels"]
        # input_ids ê¸¸ì´ì— ë§ì¶° ë¼ë²¨ ì¡°ì • (íŠ¹ìˆ˜ í† í° ê³ ë ¤)
        if len(token_labels) < len(input_ids):
            # íŒ¨ë”© ë¶€ë¶„ì€ -100 (ë¬´ì‹œ)
            labels = [-100] + [self.label2id[label] for label in token_labels] + [-100] * (len(input_ids) - len(token_labels) - 1)
        else:
            labels = [-100] + [self.label2id[label] for label in token_labels[:len(input_ids)-2]] + [-100]
        
        # ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”©
        padding_length = self.max_length - len(input_ids)
        if padding_length > 0:
            input_ids.extend([0] * padding_length)
            attention_mask.extend([0] * padding_length)
            labels.extend([-100] * padding_length)
        
        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "labels": torch.tensor(labels, dtype=torch.long)
        }

class KLUEKeywordExtractor(nn.Module):
    """KLUE-BERT ê¸°ë°˜ ë‹¨ìˆœ í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸"""
    
    def __init__(self, model_name: str = "klue/bert-base", num_labels: int = 3, dropout_rate: float = 0.3):
        super().__init__()
        
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        
        # í‚¤ì›Œë“œ í´ë˜ìŠ¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ (í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°)
        self.class_weights = torch.tensor([1.0, 3.0, 2.0])  # O, B-KEY, I-KEY
        
    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        
        sequence_output = outputs.last_hidden_state
        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output)
        
        loss = None
        if labels is not None:
            loss_fn = nn.CrossEntropyLoss(
                weight=self.class_weights.to(logits.device), 
                ignore_index=-100
            )
            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))
        
        return {"loss": loss, "logits": logits}

def create_data_loaders(labeled_data: List[Dict], test_size: float = 0.2, batch_size: int = 16):
    """ë°ì´í„° ë¡œë” ìƒì„±"""
    print(f"\nğŸ“Š ë°ì´í„°ì…‹ ë¶„í•  ë° ë°ì´í„° ë¡œë” ìƒì„±...")
    
    # Train/Validation/Test ë¶„í• 
    train_data, temp_data = train_test_split(labeled_data, test_size=test_size*2, random_state=42)
    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = KeywordDataset(train_data)
    val_dataset = KeywordDataset(val_data)
    test_dataset = KeywordDataset(test_data)
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    print(f"   í›ˆë ¨ ë°ì´í„°: {len(train_data):,}ê°œ")
    print(f"   ê²€ì¦ ë°ì´í„°: {len(val_data):,}ê°œ")
    print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data):,}ê°œ")
    print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
    
    # í›ˆë ¨ ë°ì´í„° ë¼ë²¨ ë¶„í¬ í™•ì¸
    train_labels = []
    for sample in train_data:
        train_labels.extend(sample["labels"])
    
    label_counts = pd.Series(train_labels).value_counts()
    print(f"\nğŸ“ˆ í›ˆë ¨ ë°ì´í„° ë¼ë²¨ ë¶„í¬:")
    for label, count in label_counts.items():
        print(f"   {label}: {count:,}ê°œ ({count/len(train_labels)*100:.1f}%)")
    
    return train_loader, val_loader, test_loader, train_dataset.label2id, train_dataset.id2label

def train_model(model, train_loader, val_loader, timestamp, epochs: int = 5, learning_rate: float = 2e-5):
    """ëª¨ë¸ í•™ìŠµ"""
    print(f"\nğŸš€ KLUE-BERT í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    print(f"   ì—í¬í¬: {epochs}")
    print(f"   í•™ìŠµë¥ : {learning_rate}")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"   ì‚¬ìš© ì¥ì¹˜: {device}")
    
    model.to(device)
    
    # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    total_steps = len(train_loader) * epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer, 
        num_warmup_steps=int(0.1 * total_steps), 
        num_training_steps=total_steps
    )
    
    # í•™ìŠµ ê¸°ë¡
    train_losses = []
    val_f1_scores = []
    best_f1 = 0.0
    best_model_path = f"best_model_{timestamp}.pt"
    
    for epoch in range(epochs):
        print(f"\nğŸ“š ì—í¬í¬ {epoch + 1}/{epochs}")
        
        # í›ˆë ¨ ëª¨ë“œ
        model.train()
        total_loss = 0
        all_predictions = []
        all_labels = []
        
        train_bar = tqdm(train_loader, desc=f"í›ˆë ¨ {epoch+1}")
        
        for batch in train_bar:
            # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            
            # ê·¸ë¼ë””ì–¸íŠ¸ ì´ˆê¸°í™”
            optimizer.zero_grad()
            
            # ìˆœì „íŒŒ
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs["loss"]
            
            # ì—­ì „íŒŒ
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            
            # ì†ì‹¤ ê¸°ë¡
            total_loss += loss.item()
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            predictions = torch.argmax(outputs["logits"], dim=-1)
            mask = labels != -100
            all_predictions.extend(predictions[mask].cpu().numpy())
            all_labels.extend(labels[mask].cpu().numpy())
            
            train_bar.set_postfix({"Loss": f"{loss.item():.4f}"})
        
        # ì—í¬í¬ ê²°ê³¼
        avg_loss = total_loss / len(train_loader)
        train_f1 = f1_score(all_labels, all_predictions, average='weighted')
        
        train_losses.append(avg_loss)
        
        print(f"   í›ˆë ¨ ì†ì‹¤: {avg_loss:.4f}")
        print(f"   í›ˆë ¨ F1: {train_f1:.4f}")
        
        # ê²€ì¦
        val_f1 = evaluate_model(model, val_loader, device, epoch + 1, "ê²€ì¦")
        val_f1_scores.append(val_f1)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_f1 > best_f1:
            best_f1 = val_f1
            torch.save(model.state_dict(), best_model_path)
            print(f"   ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (F1: {best_f1:.4f}) â†’ {best_model_path}")
    
    print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ! ìµœê³  ê²€ì¦ F1: {best_f1:.4f}")
    return train_losses, val_f1_scores, best_f1, best_model_path

def evaluate_model(model, data_loader, device, epoch=None, data_type="í…ŒìŠ¤íŠ¸"):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        eval_bar = tqdm(data_loader, desc=f"{data_type} í‰ê°€")
        
        for batch in eval_bar:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            predictions = torch.argmax(outputs["logits"], dim=-1)
            
            mask = labels != -100
            all_predictions.extend(predictions[mask].cpu().numpy())
            all_labels.extend(labels[mask].cpu().numpy())
    
    # ì„±ëŠ¥ ê³„ì‚°
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_predictions, average='weighted', zero_division=0
    )
    
    if epoch:
        print(f"   {data_type} F1: {f1:.4f} (ì •ë°€ë„: {precision:.4f}, ì¬í˜„ìœ¨: {recall:.4f})")
    else:
        print(f"\nğŸ” ìµœì¢… {data_type} í‰ê°€ ê²°ê³¼:")
        print(f"   ì •ë°€ë„: {precision:.4f}")
        print(f"   ì¬í˜„ìœ¨: {recall:.4f}")
        print(f"   F1 ìŠ¤ì½”ì–´: {f1:.4f}")
        
        # ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
        target_names = ["O", "B-KEY", "I-KEY"]
        report = classification_report(
            all_labels, all_predictions, 
            target_names=target_names, 
            zero_division=0
        )
        print(f"\nğŸ“Š ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        print(report)
    
    return f1

def save_model(model, tokenizer, label2id, id2label, timestamp, save_path_base: str = "klue_keyword_extractor"):
    """ëª¨ë¸ ë° ì„¤ì • ì €ì¥"""
    save_path = f"{save_path_base}_{timestamp}"
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {save_path}")
    
    try:
        os.makedirs(save_path, exist_ok=True)
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì§ì ‘ ì €ì¥ (save_pretrained ëŒ€ì‹ )
        torch.save(model.state_dict(), f"{save_path}/pytorch_model.bin")
        
        # í† í¬ë‚˜ì´ì € ì €ì¥
        tokenizer.save_pretrained(save_path)
        
        # ì„¤ì • ì •ë³´ ì €ì¥
        config = {
            "model_type": "klue_keyword_extractor",
            "base_model": "klue/bert-base",
            "num_labels": 3,
            "label2id": label2id,
            "id2label": id2label,
            "task": "keyword_extraction",
            "domain": "dementia_patient_answers",
            "labeling_scheme": "BIO",
            "classes": ["O", "B-KEY", "I-KEY"],
            "training_timestamp": timestamp,
            "created_at": datetime.now().isoformat()
        }
        
        with open(f"{save_path}/training_config.json", 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
        print(f"   ëª¨ë¸ ê²½ë¡œ: {save_path}/")
        print(f"   ì„¤ì • íŒŒì¼: {save_path}/training_config.json")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

def test_inference(model, tokenizer, test_texts: List[str], device, id2label):
    """ì‹¤ì œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ (í‚¤ì›Œë“œ ìµœëŒ€ 6ê°œ ì¶”ì¶œ ì œí•œ)""" # <<< ì£¼ì„ ìˆ˜ì •
    print(f"\nğŸ§ª ì¶”ë¡  í…ŒìŠ¤íŠ¸ (ìµœëŒ€ 6ê°œ í‚¤ì›Œë“œ)") # <<< ì¶œë ¥ ë©”ì‹œì§€ ìˆ˜ì •
    print("=" * 50)
    
    model.eval()
    
    for i, text in enumerate(test_texts):
        print(f"\ní…ŒìŠ¤íŠ¸ {i+1}: {text}")
        
        # í† í¬ë‚˜ì´ì§•
        encoding = tokenizer(
            text, 
            return_tensors='pt', 
            truncation=True, 
            padding=True,
            max_length=128
        )
        tokens = tokenizer.tokenize(text)
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            input_ids = encoding['input_ids'].to(device)
            attention_mask = encoding['attention_mask'].to(device)
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            predictions = torch.argmax(outputs['logits'], dim=-1)
        
        # ê²°ê³¼ ì²˜ë¦¬ (íŠ¹ìˆ˜ í† í° ì œì™¸)
        pred_labels = [id2label[pred.item()] for pred in predictions[0][1:len(tokens)+1]]
        
        print("í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼:")
        keywords_found = []
        current_keyword = ""
        
        # <<< ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘ >>>
        MAX_KEYWORDS = 6 # ìµœëŒ€ í‚¤ì›Œë“œ ê°œìˆ˜ ì„¤ì •

        for token, label in zip(tokens, pred_labels):
            # ì´ë¯¸ 6ê°œì˜ í‚¤ì›Œë“œë¥¼ ì°¾ì•˜ìœ¼ë©´ ë£¨í”„ ì¤‘ë‹¨
            if len(keywords_found) >= MAX_KEYWORDS:
                break

            clean_token = token.replace('##', '')
            
            if label == 'B-KEY':
                if current_keyword: # ì´ì „ í‚¤ì›Œë“œ ì™„ë£Œ
                    keywords_found.append(current_keyword)
                    # ì¶”ê°€ í›„ í‚¤ì›Œë“œ ê°œìˆ˜ í™•ì¸
                    if len(keywords_found) >= MAX_KEYWORDS:
                        current_keyword = "" # ë” ì´ìƒ ìƒˆ í‚¤ì›Œë“œ ë§Œë“¤ì§€ ì•ŠìŒ
                        break

                current_keyword = clean_token
                print(f" Â ğŸ”‘ {token} -> {label}")

            elif label == 'I-KEY' and current_keyword:
                current_keyword += clean_token
                print(f" Â â†³ {token} -> {label}")
            else:
                if current_keyword: # í‚¤ì›Œë“œ ì™„ë£Œ
                    keywords_found.append(current_keyword)
                    current_keyword = ""
        
        # ë£¨í”„ê°€ ëë‚œ í›„ ë§ˆì§€ë§‰ í‚¤ì›Œë“œ ì²˜ë¦¬
        if current_keyword and len(keywords_found) < MAX_KEYWORDS:
            keywords_found.append(current_keyword)
        # <<< ìˆ˜ì •ëœ ë¶€ë¶„ ë >>>
        
        print(f"ğŸ¯ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords_found}")

# def test_inference(model, tokenizer, test_texts: List[str], device, id2label):
#     """ì‹¤ì œ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
#     print(f"\nğŸ§ª ì¶”ë¡  í…ŒìŠ¤íŠ¸")
#     print("=" * 50)
    
#     model.eval()
    
#     for i, text in enumerate(test_texts):
#         print(f"\ní…ŒìŠ¤íŠ¸ {i+1}: {text}")
        
#         # í† í¬ë‚˜ì´ì§•
#         encoding = tokenizer(
#             text, 
#             return_tensors='pt', 
#             truncation=True, 
#             padding=True,
#             max_length=128
#         )
#         tokens = tokenizer.tokenize(text)
        
#         # ì˜ˆì¸¡
#         with torch.no_grad():
#             input_ids = encoding['input_ids'].to(device)
#             attention_mask = encoding['attention_mask'].to(device)
            
#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)
#             predictions = torch.argmax(outputs['logits'], dim=-1)
        
#         # ê²°ê³¼ ì²˜ë¦¬ (íŠ¹ìˆ˜ í† í° ì œì™¸)
#         pred_labels = [id2label[pred.item()] for pred in predictions[0][1:len(tokens)+1]]
        
#         print("í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼:")
#         keywords_found = []
#         current_keyword = ""
        
#         for token, label in zip(tokens, pred_labels):
#             clean_token = token.replace('##', '')
            
#             if label == 'B-KEY':
#                 if current_keyword:  # ì´ì „ í‚¤ì›Œë“œ ì™„ë£Œ
#                     keywords_found.append(current_keyword)
#                 current_keyword = clean_token
#                 print(f"  ğŸ”‘ {token} -> {label}")
#             elif label == 'I-KEY' and current_keyword:
#                 current_keyword += clean_token
#                 print(f"  â†³ {token} -> {label}")
#             else:
#                 if current_keyword:  # í‚¤ì›Œë“œ ì™„ë£Œ
#                     keywords_found.append(current_keyword)
#                     current_keyword = ""
        
#         if current_keyword:  # ë§ˆì§€ë§‰ í‚¤ì›Œë“œ ì²˜ë¦¬
#             keywords_found.append(current_keyword)
        
#         print(f"ğŸ¯ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords_found}")

def plot_training_history(train_losses, val_f1_scores, timestamp):
    """í•™ìŠµ íˆìŠ¤í† ë¦¬ ì‹œê°í™”"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # ì†ì‹¤ ê·¸ë˜í”„
    ax1.plot(train_losses, 'b-', label='Training Loss')
    ax1.set_title('Training Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    # F1 ìŠ¤ì½”ì–´ ê·¸ë˜í”„
    ax2.plot(val_f1_scores, 'r-', label='Validation F1')
    ax2.set_title('Validation F1 Score')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('F1 Score')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨í•œ íŒŒì¼ëª…
    history_filename = f'training_history_{timestamp}.png'
    plt.savefig(history_filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“Š í•™ìŠµ íˆìŠ¤í† ë¦¬ ê·¸ë˜í”„ ì €ì¥: {history_filename}")

def main():
    """ê°œì„ ëœ 4ë‹¨ê³„ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ê°œì„ ëœ 4ë‹¨ê³„: KLUE-BERT í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ í•™ìŠµ")
    print("=" * 60)
    print("ë¼ë²¨ë§ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ìˆœ í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ í•™ìŠµ")
    print("ë¼ë²¨ ì²´ê³„: O (ë¹„í‚¤ì›Œë“œ), B-KEY (í‚¤ì›Œë“œ ì‹œì‘), I-KEY (í‚¤ì›Œë“œ ë‚´ë¶€)")
    print("=" * 60)
    
    # 0. ì´ì „ í•™ìŠµ ê²°ê³¼ ì •ë¦¬
    cleanup_previous_training()
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamp = generate_timestamp_suffix()
    print(f"\nğŸ• í•™ìŠµ ì„¸ì…˜ ID: {timestamp}")
    
    # 1. ë¼ë²¨ë§ëœ ë°ì´í„° ë¡œë“œ
    labeled_data = load_labeled_data()
    if not labeled_data:
        return
    
    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
    print(f"\nğŸ”§ KLUE-BERT í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    
    # 3. ë°ì´í„° ë¡œë” ìƒì„±
    train_loader, val_loader, test_loader, label2id, id2label = create_data_loaders(
        labeled_data, test_size=0.2, batch_size=16
    )
    
    # 4. ëª¨ë¸ ìƒì„± (ë§¤ë²ˆ ìƒˆë¡œ ì´ˆê¸°í™”)
    model = KLUEKeywordExtractor(num_labels=3)
    print(f"\nğŸ—ï¸ KLUE-BERT í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ìƒì„± ì™„ë£Œ (ìƒˆë¡œ ì´ˆê¸°í™”ë¨)")
    
    # 5. ëª¨ë¸ í•™ìŠµ
    train_losses, val_f1_scores, best_f1, best_model_path = train_model(
        model, train_loader, val_loader, timestamp, epochs=6, learning_rate=2e-5
    )
    
    # 6. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.load_state_dict(torch.load(best_model_path))
    model.to(device)
    print(f"\nğŸ“¥ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ: {best_model_path}")
    
    # 7. ìµœì¢… í‰ê°€
    final_f1 = evaluate_model(model, test_loader, device)
    
    # 8. ëª¨ë¸ ì €ì¥
    save_model(model, tokenizer, label2id, id2label, timestamp)
    
    # 9. ì¶”ë¡  í…ŒìŠ¤íŠ¸
    test_texts = [
        "ëŒ€í•™ì—ì„œ ì²˜ìŒìœ¼ë¡œ ì»´í“¨í„°ë¥¼ ì‚¬ìš©í•´ ë³´ì•˜ë˜ ê¸°ì–µì´ ìˆì–´ìš”. ì‹ ê¸°í•˜ê³  ì–´ìƒ‰í–ˆì–´ìš”.",
        "ìì‹ë“¤ì´ ëŒ€í•™ì— í•©ê²©í•œ ìˆœê°„ì„ ìŠì§€ ëª»í•´ìš”. ì„œë¡œë¥¼ ëŒì–´ì•ˆê³  í‘í‘ ìš¸ì—ˆë˜ ê²ƒ ê°™ì•„ìš”.", 
        "ë‚¨í¸ì´ ì—°ë½ì„ ë°›ìœ¼ë©´ ê¼­ ì²­í˜¼í•´ì•¼ ê² ë‹¤ëŠ” ë§ˆìŒìœ¼ë¡œ ì—°ë½í–ˆì–´ìš”. ë‹¤í–‰ì´ ì²­í˜¼ì— ì„±ê³µí•˜ê³  ê²°í˜¼ê¹Œì§€ í–ˆë„¤ìš”.",
        "ì‚¶ì´ í˜ë“¤ì–´ë„ ë‹¤ì‹œ ì¼ì–´ì„¤ ìš©ê¸°ê°€ í•„ìš”í•˜ë‹¤ëŠ”ê±¸ ì•„ì´ë“¤ì—ê²Œ í•­ìƒ ê°•ì¡°í•´ì¤¬ë˜ ê²ƒ ê°™ì•„ìš”."
    ]
    test_inference(model, tokenizer, test_texts, device, id2label)
    
    # 10. í•™ìŠµ íˆìŠ¤í† ë¦¬ ì‹œê°í™”
    plot_training_history(train_losses, val_f1_scores, timestamp)
    
    # 11. ì™„ë£Œ ë³´ê³ ì„œ
    print(f"\nğŸ‰ ê°œì„ ëœ 4ë‹¨ê³„ ì™„ë£Œ ë³´ê³ ì„œ")
    print("=" * 40)
    print(f"ğŸ• í•™ìŠµ ì„¸ì…˜ ID: {timestamp}")
    print(f"âœ… í›ˆë ¨ ë°ì´í„°: {len(labeled_data):,}ê°œ")
    print(f"âœ… ìµœê³  ê²€ì¦ F1: {best_f1:.4f}")
    print(f"âœ… ìµœì¢… í…ŒìŠ¤íŠ¸ F1: {final_f1:.4f}")
    print(f"âœ… ëª¨ë¸ ì €ì¥: klue_keyword_extractor_{timestamp}/")
    print(f"âœ… ìµœê³  ëª¨ë¸ íŒŒì¼: {best_model_path}")
    print(f"âœ… í•™ìŠµ ê³¡ì„ : training_history_{timestamp}.png")
    print(f"âœ… ë¼ë²¨ ì²´ê³„: 3ê°œ í´ë˜ìŠ¤ (O, B-KEY, I-KEY)")
    
    print(f"\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"1. ì‹¤ì œ ì„œë¹„ìŠ¤ì— ëª¨ë¸ í†µí•©")
    print(f"2. ë” ë§ì€ ë¼ë²¨ë§ ë°ì´í„°ë¡œ ì„±ëŠ¥ ê°œì„ ")
    print(f"3. ëª¨ë¸ ìµœì í™” ë° ê²½ëŸ‰í™”")
    print(f"4. ë¼ë²¨ë§ í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ ")
    
    print(f"\nğŸ’¾ ìƒì„±ëœ íŒŒì¼ë“¤:")
    print(f"   ğŸ“ klue_keyword_extractor_{timestamp}/")
    print(f"   ğŸ† {best_model_path}")
    print(f"   ğŸ“Š training_history_{timestamp}.png")
    
    return model, tokenizer, final_f1, timestamp

if __name__ == "__main__":
    main()
    
